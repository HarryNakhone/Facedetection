{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a5a3e-3eb9-45f1-a2bb-0f4a766195d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f9316f-9ed9-4dc8-a4b1-99e06073c161",
   "metadata": {},
   "source": [
    "## 1. Collect img through webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452a489b-eb65-41fd-ba69-62e9093f9398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bf8f9a-5f51-4e84-b5c1-536d734f9171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40f24e-daf8-4d0c-95e9-b12132070641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import uuid\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb71bfb-bece-49b5-9afa-ae1f52858dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = os.path.join('data','images')\n",
    "num_img = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99878841-1464-4fd3-82b8-880d9623d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "facefolder = \"YOUR FULL PATH\\OF THIS WORKING DIRECTORY\\\"\n",
    "data  = \"data\"\n",
    "img_lbl = [\"images\", \"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e06cd49-cd04-409f-a29f-6313b37f0ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run this cell if there is no data folder /// OPTIONAL ///\n",
    "if not os.path.exists(os.path.join(facefolder, data)):\n",
    "    os.makedirs(os.path.join(facefolder, data))\n",
    "    if not os.listdir(os.path.join(facefolder, data)):\n",
    "        for a in img_lbl:\n",
    "            os.makedirs(os.path.join(facefolder, data, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10402e53-bde3-461f-b035-d7ce93375002",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "for imgnum in range(num_img):\n",
    "    print('Collecting image {}'.format(imgnum))\n",
    "    ret, frame = cap.read()\n",
    "    imgname = os.path.join(IMAGES_PATH, f'{str(uuid.uuid1())}.jpg')\n",
    "    cv2.imwrite(imgname, frame)\n",
    "    cv2.imshow('frame', frame)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  ## allows us to break out of the loop\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc8e6dc-2135-4022-b6d8-982fbf2ef3a7",
   "metadata": {},
   "source": [
    "### 1.2 Annotate the img with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bac14b-a4a1-44cf-951b-05964b9c6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelme "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5014700-6d07-4884-a593-a3e6648351c8",
   "metadata": {},
   "source": [
    "## 2. Review the Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a58f4c-d94c-48c8-a68e-fabebd492137",
   "metadata": {},
   "source": [
    "### Limit memory growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb17f82-db35-4adc-9a8c-19e7cb7bb723",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6e600-f8b3-4aa7-8030-348e288c7215",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599cf6db-86b5-4201-916c-a20c7fa1dffa",
   "metadata": {},
   "source": [
    "#### Load Img into Tensorflow data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1987af01-eee5-4637-a150-2a9e3a82eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.data.Dataset.list_files('data\\\\images\\\\*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3e120-59b1-414a-aa98-baa78655e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f072aae-3bdb-4e71-b1bd-2c8084a4fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(x):\n",
    "    byte_img = tf.io.read_file(x)  ### read the file path and then return byte encode img\n",
    "    image = tf.io.decode_jpeg(byte_img) ### decode\n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f3bb7-8dba-46bc-a752-6425b8b1d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.map(load_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f0c09e-c0ff-40fb-898a-b8c0d4e45d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    " img.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8390cc37-3629-4d68-a4b2-0aaea3265e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_generator = img.batch(4).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceecf4a-e841-45a6-b758-1659c21d9aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images = img_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071de58d-4f3b-4fbf-b3c8-d1cd70dbc0a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20)) \n",
    "for idx, image in enumerate(plot_images):\n",
    "    ax[idx].imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5a26d6-5488-4eb8-aeca-0a0d99267978",
   "metadata": {},
   "source": [
    "### Partition Unaug Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a755a0c-b9ac-4170-aea3-cde445f500bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = [\"train\", \"test\", \"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f17687f-de1b-494a-b63c-8446129378e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run this cell to create train, test, and validation folders with images and labels in it.\n",
    "\n",
    "if not os.path.exists(os.path.join(facefolder, data, partition[0])):\n",
    "    for a in partition:\n",
    "        os.makedirs(os.path.join(facefolder, data, a))\n",
    "        for e in img_lbl:\n",
    "            os.makedirs(os.path.join(facefolder, data, a, e))\n",
    "else:\n",
    "    print(\"Folders are existed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0714296b-ca71-4947-a424-37ab3c5868a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split data 70% train, 15% test and 15% Val\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Paths to the original image directory\n",
    "images_dir = 'data/images'\n",
    "\n",
    "# Destination directories for the images\n",
    "train_dir = 'data/train/images'\n",
    "val_dir = 'data/val/images'\n",
    "test_dir = 'data/test/images'\n",
    "\n",
    "all_images = sorted(os.listdir(images_dir))\n",
    "\n",
    "random.shuffle(all_images)\n",
    "total_images = len(all_images)\n",
    "train_split = int(0.7 * total_images)\n",
    "val_split = int(0.15 * total_images)\n",
    "\n",
    "train_files = all_images[:train_split]  ### from beginning to the 70%\n",
    "val_files = all_images[train_split:train_split + val_split]  ### from 70% to 85%\n",
    "test_files = all_images[train_split + val_split:] ### last 15%\n",
    "\n",
    "def move_images(file_list, dest_dir):\n",
    "    for file_name in file_list:\n",
    "        img_src = os.path.join(images_dir, file_name)\n",
    "        img_dest = os.path.join(dest_dir, file_name)\n",
    "\n",
    "        shutil.move(img_src, img_dest)\n",
    "\n",
    "move_images(train_files, train_dir)\n",
    "move_images(val_files, val_dir)\n",
    "move_images(test_files, test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf197b-602d-499d-9bd5-0aabc3aeed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Moveing labels from orignal folder into each: train, test and validation\n",
    "\n",
    "for folder in ['train', 'test', 'val']:\n",
    "    for file in os.listdir(os.path.join('data', folder, 'images')):\n",
    "        filename = file.split('.')[0]+'.json'\n",
    "        existing_filepath = os.path.join('data', 'labels', filename)\n",
    "        if os.path.exists(existing_filepath):\n",
    "            new_filepath  =os.path.join('data', folder, 'labels', filename)\n",
    "            os.replace(existing_filepath, new_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8961a2e8-e2d6-488f-bc1c-01b30f523b44",
   "metadata": {},
   "source": [
    "### Apply image Augmentation on Images and Labels on 1 img as example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131bfd86-bb82-4b9e-8ac2-ffc1c170d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = albu.Compose([\n",
    "    albu.RandomCrop(width=450, height = 450),\n",
    "    albu.HorizontalFlip(p=0.5),\n",
    "    albu.RandomBrightnessContrast(p=0.2),\n",
    "    albu.RandomGamma(p=0.2),\n",
    "    albu.RGBShift(p =0.2),\n",
    "    albu.VerticalFlip(p=0.5)\n",
    "], bbox_params=albu.BboxParams(format='albumentations', label_fields = ['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b74dd-3146-4601-8ace-042bb666587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join('data', 'train', 'images', 'ANY IMAGE .jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a331848-aee1-4a5f-9f86-936eba6734f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'train', 'labels', 'YOUR JSON FILE .json'), 'r') as f:\n",
    "    label = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e19e9-6e63-411d-ae7d-6b6bf2dddcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = label['shapes'][0]\n",
    "len(shape['points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c9f2b2-4009-4cba-8353-99ecd2d24d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f919a64-c982-4bc1-90c0-71e904886745",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = [0,0,0,0]\n",
    "coords[0] = label['shapes'][0]['points'][0][0]\n",
    "coords[1] = label['shapes'][0]['points'][0][1]\n",
    "coords[2] = label['shapes'][0]['points'][1][0]\n",
    "coords[3] = label['shapes'][0]['points'][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4051e8e7-180b-449e-9dfc-227542b051ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords ### Raw Pascal Voc format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c02880a-db72-470e-bbff-b60475025c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = list(np.divide(coords, [640,480,640,480]))  ### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d4742e-acad-429d-bcbd-d2b27319d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords  ### Now be Albumentations format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb5151-845d-4554-9b8d-8aeeee051e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_img = augmentor(image = img, bboxes = [coords], class_labels= ['face'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f47a32-39ff-4cfa-befc-9e55cf6c1e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_img['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d28aa-6a17-4300-935c-607505c2d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(augmented_img['image'],\n",
    "              tuple(np.multiply(augmented_img['bboxes'][0][:2], [450, 450]).astype(int)),\n",
    "              tuple(np.multiply(augmented_img['bboxes'][0][2:], [450, 450]).astype(int)),\n",
    "                     (255,0,0), 2)  ### 2 is the thickness of the rectangle\n",
    "plt.imshow(augmented_img['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f1afde-eb54-4fb7-bff3-326bd1818124",
   "metadata": {},
   "source": [
    "### Build Augmentation Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a4b95-9755-4b91-85c8-0a7d425533e1",
   "metadata": {},
   "source": [
    "##### Create augment folder for augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb8bf3-e247-49ca-bc98-c832890f80d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUG_PATH = \"aug_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18cad9-f321-4faa-b39a-2417635bc34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(AUG_PATH):\n",
    "    os.makedirs(os.path.join(facefolder,AUG_PATH))\n",
    "\n",
    "if not os.path.exists(os.path.join(facefolder, AUG_PATH, partition[0])):\n",
    "    for a in partition:\n",
    "        os.makedirs(os.path.join(facefolder, AUG_PATH , a))\n",
    "        for e in img_lbl:\n",
    "            os.makedirs(os.path.join(facefolder, AUG_PATH , a, e))\n",
    "else:\n",
    "    print(\"Folders are existed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a4ccdc-83b5-45a8-afb2-72e2819f7ad6",
   "metadata": {},
   "source": [
    "##### Build Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f7a71-f2e9-4470-8eeb-c236c7441969",
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition in ['train', 'test', 'val']:\n",
    "    for image in os.listdir(os.path.join('data', partition, 'images')):\n",
    "        img = cv2.imread(os.path.join('data', partition, 'images', image))\n",
    "\n",
    "        coords = [0, 0, 0.00001, 0.00001] #### set default coords for img that does not have label\n",
    "        label_path = os.path.join('data', partition, 'labels', f'{image.split(\".\")[0]}.json')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)   #### Load the json file to get class and coordinates\n",
    "            if 'shapes' in label and len(label['shapes']) > 0:\n",
    "                shape = label['shapes'][0]\n",
    "                if 'points' in shape and len(shape['points']) >= 2:\n",
    "                    coords[0] = label['shapes'][0]['points'][0][0]\n",
    "                    coords[1] = label['shapes'][0]['points'][0][1]\n",
    "                    coords[2] = label['shapes'][0]['points'][1][0]\n",
    "                    coords[3] = label['shapes'][0]['points'][1][1]\n",
    "                    coords = list(np.divide(coords, [640, 480, 640, 480])) ### from pascal voc to albumentation\n",
    "                else:\n",
    "                    print(f\"Warning: Not enough points in {label_path}\")\n",
    "            else:\n",
    "                print(f\"Warning: No shapes in {label_path}\")\n",
    "            \n",
    "\n",
    "        \n",
    "        try:\n",
    "            for x in range(60):\n",
    "                augmented = augmentor(image = img, bboxes=[coords], class_labels=['face'])\n",
    "                cv2.imwrite(os.path.join('aug_data', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'), augmented['image'])\n",
    "\n",
    "                annotation = {}\n",
    "                annotation['image'] = image\n",
    "\n",
    "                if os.path.exists(label_path):\n",
    "                    if len(augmented['bboxes']) ==0:\n",
    "                        annotation['bbox'] = [0,0,0,0]\n",
    "                        annotation['class'] = 0\n",
    "                    else:\n",
    "                        annotation['bbox'] = augmented['bboxes'][0]\n",
    "                        annotation['class'] = 1\n",
    "                else:\n",
    "                    annotation['bbox'] = [0,0,0,0]\n",
    "                    annotation['class'] = 0\n",
    "\n",
    "                with open(os.path.join('aug_data', partition, 'labels', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n",
    "                    json.dump(annotation, f)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1a2c87-2012-48e5-b0d0-f86fbe923750",
   "metadata": {},
   "source": [
    "### Load Augmented Images to Tenforflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d6995-c529-45c4-8dcb-4a5c8fcb678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.data.Dataset.list_files('aug_data\\\\train\\\\images\\\\*.jpg', shuffle= False)\n",
    "train_images = train_images.map(load_image)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (120, 120)))\n",
    "train_images = train_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aca637-e2d2-4cff-a11b-e75f9efb375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.data.Dataset.list_files('aug_data\\\\test\\\\images\\\\*.jpg', shuffle= False)\n",
    "test_images = test_images.map(load_image)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (120, 120)))\n",
    "test_images = test_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600369f8-15ee-4162-87f4-957a6dadb544",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = tf.data.Dataset.list_files('aug_data\\\\val\\\\images\\\\*.jpg', shuffle= False)\n",
    "val_images = val_images.map(load_image)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (120, 120)))\n",
    "val_images = val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea1933d-2795-49fd-b977-e8c72262df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba57e77-db79-44ff-8893-f1d6e0220e6b",
   "metadata": {},
   "source": [
    "## Gte Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50302d03-70bb-4cc5-8607-24becbec621f",
   "metadata": {},
   "source": [
    "#### Build label loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6eac05-a7f3-41c1-b59b-f4254be3b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_path):\n",
    "    with open(label_path.numpy(), 'r', encoding = \"utf-8\") as f:\n",
    "        label = json.load(f)\n",
    "    return [label['class']], label['bbox']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caef1ba-a0a9-4dfd-9293-f425b9284886",
   "metadata": {},
   "source": [
    "#### Load labels to tensorflow Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d479b-40fb-425b-9d17-48f6b907da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.data.Dataset.list_files('aug_data\\\\train\\\\labels\\\\*.json', shuffle= False)\n",
    "train_labels = train_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f304a-1db7-4ac6-a517-0c2d075c02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = tf.data.Dataset.list_files('aug_data\\\\test\\\\labels\\\\*.json', shuffle= False)\n",
    "test_labels = test_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de685c-0134-4d40-ba6a-776bec9a36fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = tf.data.Dataset.list_files('aug_data\\\\val\\\\labels\\\\*.json', shuffle= False)\n",
    "val_labels = val_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc715e6a-4dbe-4c6a-bfc1-40af053f5e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66af7f-a729-4995-8aa3-48221b464a26",
   "metadata": {},
   "source": [
    "## Combine Label and Image Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6825fd62-70d7-44af-b7bf-20b1c4168d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_images), len(train_labels), len(test_images), len(test_labels), len(val_images), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f584cc-b043-4587-89e0-03f87fc88f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.zip((train_images, train_labels))\n",
    "train = train.shuffle(5000)\n",
    "train = train.batch(8)\n",
    "train = train.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee143df-3266-4872-9517-9e5673205bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.data.Dataset.zip((train_images, train_labels))\n",
    "test = test.shuffle(5000)\n",
    "test = test.batch(8)\n",
    "test = test.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b874b06b-a06a-4029-88db-e2ba717364c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = tf.data.Dataset.zip((train_images, train_labels))\n",
    "val = val.shuffle(5000)\n",
    "val = val.batch(8)\n",
    "val = val.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305505f3-ae4f-4a0a-99d6-62ad41303fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5084c8-7047-4281-a089-450bb890b823",
   "metadata": {},
   "source": [
    "##### View Img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc054efa-0e70-4362-80a7-5b526122ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = train.as_numpy_iterator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71acba90-4ffb-404c-8de5-5f3300727a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = data_samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c845c405-2e08-464c-a870-dbc9a9d8c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = res[0][idx].copy()\n",
    "    sample_coords = res[1][1][idx]\n",
    "    \n",
    "    cv2.rectangle(sample_image, \n",
    "                  tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                  tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                        (255,0,0), 2)\n",
    "\n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f44181-6cca-412c-a7e2-ea2def392d0b",
   "metadata": {},
   "source": [
    "## Build Deeping learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a649e8ce-b5f2-4cd4-8c8d-f27f254c16aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a29dcc6-7dfb-4545-84a3-bb8dc4cee542",
   "metadata": {},
   "source": [
    "### Download VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888a9c10-85de-4b15-9fd1-fead2e45f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8383307f-68be-4138-9b12-a2a74fa4c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9421823e-1481-4812-b1bc-ea044d8eabcd",
   "metadata": {},
   "source": [
    "#### Build instance of Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a8ab8-8d2f-42b6-bf46-66528c5e2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_layer = Input(shape=(120,120, 3))\n",
    "\n",
    "    vgg = VGG16(include_top=False)(input_layer)  #### Note: include_top = False means we are dropping the connected layer\n",
    "\n",
    "    f1 = GlobalMaxPooling2D()(vgg) #### For classification\n",
    "    class1 = Dense(2048, activation='relu')(f1)\n",
    "    class2 = Dense(1, activation = 'sigmoid')(class1)\n",
    " \n",
    "    f2 = GlobalMaxPooling2D()(vgg) #### For Box Model \n",
    "    regress1 = Dense(2048, activation = 'relu')(f2)\n",
    "    regress2 = Dense(4, activation = 'sigmoid')(regress1)\n",
    "\n",
    "    facetracker  =Model(inputs = input_layer, outputs = [class2, regress2])\n",
    "    return facetracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6144d331-c6b0-4ceb-ae73-6354bf9620bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd63e8e-c587-4bfa-9bd1-3b993b4183e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1145057-4530-4098-9099-33ee3a80e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y =train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd099ed-3e7f-4486-8072-15a0745bdd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda27fd-cfa5-4a80-a084-e2bc14f47578",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords =facetracker.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ca6c3d-fd12-4196-b146-ea78c44f21dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7771e003-a8bd-4349-9b93-5cb179775a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ce002-98a2-445a-9b2b-c40f1c57c612",
   "metadata": {},
   "source": [
    "## Create Losses and Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1988ed-c173-4f94-9920-715f97664518",
   "metadata": {},
   "source": [
    "###### Define how much learning rate will drop after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac2fa8-e854-4fd5-85f1-7201ee8c40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = len(train)\n",
    "lr_decay = (1./0.75 -1)/batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7597b-e2ad-4fda-a80b-55962ea50f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)  ##### Optimizer for backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae497532-a9c4-495c-85f3-28ebcbe34d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true, yhat):\n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:, :2] - yhat[:, :2]))\n",
    "    \n",
    "    h_true = y_true[:, 3] - y_true[:,1] ### Calculate actual height and width of the box\n",
    "    w_true = y_true[:, 2] - y_true[:, 0]\n",
    "\n",
    "    h_pred = yhat[:,3] - yhat[:, 1]  ### Calculate predicted height and width of the box\n",
    "    w_pred = yhat[:,2] - yhat[:, 0]\n",
    "\n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true - h_pred))\n",
    "\n",
    "    return delta_coord + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f42aaf-ef62-4e18-ab17-19d016f54bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss = tf.keras.losses.BinaryCrossentropy()\n",
    "regressloss = localization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4573faee-9173-47e1-b185-ce1aab61a1e8",
   "metadata": {},
   "source": [
    "#### Test Loss Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061b18f-fc72-44aa-96f5-4ac7a2387d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_loss(y[1], coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89758d05-742e-43a7-bdf0-09aebc0ff94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss(y[0], classes).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76354c2-641f-4b2a-a877-654c3cdb4e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressloss(y[1], coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3384de2a-ed0d-42b4-a839-d85bbcfaba97",
   "metadata": {},
   "source": [
    "# Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42579d4b-310c-4e74-8ff1-7ff55b0e93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### When creating model class from keras, always define __init__, compile, train_step, and call\n",
    "class FaceTracker(Model):\n",
    "    def __init__(self, facetracker,  **kwargs):   ### Pass in initial params\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = facetracker\n",
    "\n",
    "    def compile(self, opt, classloss, localizationloss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.closs = classloss\n",
    "        self.lloss = localizationloss\n",
    "        self.opt = opt\n",
    "\n",
    "    def train_step(self, batch, *kwargs):\n",
    "        X,y =  batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            classes, coords = self.model(X, training=True)  ### Make prediction\n",
    "\n",
    "            batch_classloss = self.closs(y[0], classes)   ### Cal loss\n",
    "            batch_localizationloss = self.lloss(tf.cast(y[1],tf.float32), coords) ### Cal loss\n",
    "\n",
    "            total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "\n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables) ### Cal gradient\n",
    "\n",
    "        opt.apply_gradients(zip(grad, self.model.trainable_variables)) ### Apply backpropagation\n",
    "\n",
    "        return {\"total_loss\": total_loss, \"class_loss\": batch_classloss, \"regress_loss\": batch_localizationloss}\n",
    "\n",
    "    def test_step(self, batch, **kwargs):\n",
    "        X, y  =batch\n",
    "\n",
    "        classes, coords  =self.model(X, training  = False)\n",
    "\n",
    "        batch_classloss = self.closs(y[0], classes)\n",
    "        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "        total_loss  =batch_localizationloss+0.5*batch_classloss\n",
    "\n",
    "        return {\"total_loss\": total_loss, \"class_loss\": batch_classloss, \"regress_loss\": batch_localizationloss}\n",
    "\n",
    "    def call(self, X, **kwargs):\n",
    "        return self.model(X, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d1c7b5-da93-43a0-a0fb-af5c8868d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  FaceTracker(facetracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc32a8d-f4c8-4ec6-9c44-9b3f91748161",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(opt, classloss, regressloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8b683-dc12-44ec-a366-696d5ec98e18",
   "metadata": {},
   "source": [
    "#### Log Dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664adff2-9c42-4965-b3f1-9af6f37ef08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e1a8e-9220-4010-bd7f-5774081789c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback  = tf.keras.callbacks.TensorBoard(log_dir = logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0606d0-1a00-4eaf-b782-73ec20f8f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist  =model.fit(train, epochs = 40, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b41685-4bc7-4d2f-a390-612ab11a4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4866ee-9c8e-4acb-8b8d-f0b1381294c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n",
    "\n",
    "ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n",
    "ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n",
    "ax[0].title.set_text('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(hist.history['class_loss'], color='teal', label='class loss')\n",
    "ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n",
    "ax[1].title.set_text('Classification Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n",
    "ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n",
    "ax[2].title.set_text('Regression Loss')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c3265-5241-4477-9109-70981e46e3aa",
   "metadata": {},
   "source": [
    "# Predicting using a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263abf59-4810-49b8-97fd-38cd13d41801",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dat = test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed5e324-3844-49f4-bdba-40a6fee47445",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_dat.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba2444c-a69c-49e4-8c0d-4cc0b288cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = facetracker.predict(test_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ddaae-8778-4f36-994c-485cd5143115",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = test_sample[0][idx].copy()\n",
    "    sample_coords = yhat[1][idx]\n",
    "    \n",
    "    if yhat[0][idx] > 0.9:\n",
    "        cv2.rectangle(sample_image, \n",
    "                      tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "    \n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d82d889-d561-4503-a257-043b56fff144",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71cd261-42d1-4468-bb82-d5746a4dd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12636244-80df-4706-9a84-dd702f7eac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker.save('facetracker2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40968bab-5869-4095-a45b-08868e18aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = load_model('facetracker2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3726163b-92b1-41b3-aa03-6f7eb5d09294",
   "metadata": {},
   "source": [
    "## Testing real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67394815-b30e-416a-ad9f-5b3958fd8441",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    _ , frame = cap.read()\n",
    "    frame = frame[50:500, 50:500,:]\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120,120))\n",
    "    \n",
    "    yhat = facetracker.predict(np.expand_dims(resized/255,0))\n",
    "    sample_coords = yhat[1][0]\n",
    "    \n",
    "    if yhat[0] > 0.5: \n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.multiply(sample_coords[:2], [450,450]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [450,450]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int), \n",
    "                                    [0,-30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                    [80,0])), \n",
    "                            (255,0,0), -1)\n",
    "        \n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [450,450]).astype(int),\n",
    "                                               [0,-5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('EyeTrack', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4db20b-e160-4aa0-96a1-09cb89c0783c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "objdetec",
   "language": "python",
   "name": "objenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
